{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6a6cc7",
   "metadata": {},
   "source": [
    "# AI-Assisted Financial Analysis Report\n",
    "\n",
    "This Google Colab notebook demonstrates how to leverage a large language model (LLM) to assist in building a financial analysis report.\n",
    "It uses the project instructions and lecture notes from the **Financial Accounting for Managers** course to guide the model and generate a structured report.\n",
    "\n",
    "The workflow will:\n",
    "\n",
    "1. Install required libraries from Hugging Face.\n",
    "2. Load the provided Quarto (`.qmd`) files containing the project instructions and lecture topics.\n",
    "3. Summarise long texts using a pre‑trained summarisation model.\n",
    "4. Build a prompt that combines the key points from the instructions and lectures.\n",
    "5. Use a text generation model to draft a report with six sections: Company description, Financing, Investing, Operating performance, Forecast, and Conclusions.\n",
    "6. Save the generated report to a local file for further editing and review.\n",
    "\n",
    "**Note**: This notebook is meant as a pedagogical example.  You can adapt the choice of model or parameters depending on your needs.  Colab provides internet access so installing and loading models from the Hugging Face Hub should work.  Make sure you have uploaded the `.qmd` files (project instructions and topics) into the Colab environment or adjust the file paths accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4af2d",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First we install and import the necessary libraries.  The `transformers` library from Hugging Face provides easy‑to‑use pipelines for text summarisation and generation.\n",
    "\n",
    "Executing the cell below will install the dependencies (it may take a few minutes when run for the first time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03de2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face Transformers (uncomment if running for the first time)\n",
    "\n",
    "#%pip install torch --quiet\n",
    "#%pip install ipywidgets --quiet\n",
    "#%pip install -q transformers sentencepiece\n",
    "\n",
    "# Import required modules\n",
    "from transformers import pipeline\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c3c14",
   "metadata": {},
   "source": [
    "## 2. Load project instructions and lecture topics\n",
    "\n",
    "The project instructions and lecture topics are stored in Quarto (`.qmd`) files.\n",
    "To use them in Colab, you can either upload the files manually (`File` → `Upload`) or mount them from Google Drive.\n",
    "Make sure the file names below match the uploaded files.\n",
    "\n",
    "Below we define a helper function to read a text file and return its contents as a string.  Then we read the project instructions and each of the six lecture topic files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e4dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction file preview:\n",
      "---\n",
      "title: \"Financial Accounting for Managers: Final Project\"\n",
      "author: Marcelo Ortiz M.\n",
      "date: November 2024\n",
      "format:\n",
      "  pdf:\n",
      "    toc: true\n",
      "    number-sections: false\n",
      "---\n",
      "\n",
      "# General instructions\n",
      "\n",
      "In groups of 3 or 4 students, pursue a financial analysis of a publicly-listed company from @tbl-companies. This analysis must be based on the latest annual reports available, and complemented with additional information when needed.\n",
      "\n",
      "| Industry                              | Company                 |\n",
      "|----\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Helper function to read a text file\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "# Change to the directory where your .qmd files are located\n",
    "os.chdir('/home/miortiz/Documents/GitHub/lecture_financial_statements_analysis/')\n",
    "# Paths to the .qmd files (adjust these if your files have different names or locations)\n",
    "instruction_path = 'project_instructions.qmd'\n",
    "topic_files = [\n",
    "    'topic_1_foundations.qmd',\n",
    "    'topic_2_financing.qmd',\n",
    "    'topic_3_investing.qmd',\n",
    "    'topic_4_operating.qmd',\n",
    "    'topic_5_cashflows.qmd',\n",
    "    'topic_6_forecasting.qmd'\n",
    "]\n",
    "\n",
    "# Read the project instructions\n",
    "instructions_text = read_file(instruction_path)\n",
    "\n",
    "# Read and concatenate the lecture topics into a single string\n",
    "topics_text = \"\\n\".join([read_file(path) for path in topic_files])\n",
    "\n",
    "# Show a short preview of the instruction file (optional)\n",
    "print('Instruction file preview:')\n",
    "print(instructions_text[:500] + '\\n...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d362b11",
   "metadata": {},
   "source": [
    "## 3. Summarise long documents\n",
    "\n",
    "Language models typically have a limit on the number of tokens they can process at once.  To summarise long documents like our instruction file or lecture notes, we split the text into manageable chunks, summarise each chunk, and then optionally summarise the summaries.\n",
    "\n",
    "The function below uses a summarisation pipeline with a BART model (`facebook/bart-large-cnn`) to produce concise summaries.  Feel free to experiment with other models available on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48909716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 250, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 250, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 250, but your input_length is only 244. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=122)\n",
      "Your max_length is set to 250, but your input_length is only 244. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=122)\n",
      "Your max_length is set to 250, but your input_length is only 215. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=107)\n",
      "Your max_length is set to 250, but your input_length is only 215. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=107)\n",
      "Your max_length is set to 250, but your input_length is only 225. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n",
      "Your max_length is set to 250, but your input_length is only 225. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n",
      "Your max_length is set to 250, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Your max_length is set to 250, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "Your max_length is set to 250, but your input_length is only 224. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n",
      "Your max_length is set to 250, but your input_length is only 224. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n",
      "Your max_length is set to 250, but your input_length is only 145. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n",
      "Your max_length is set to 250, but your input_length is only 145. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n",
      "Your max_length is set to 250, but your input_length is only 223. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=111)\n",
      "Your max_length is set to 250, but your input_length is only 223. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of project instructions:\n",
      " In groups of 3 or 4 students, pursue a financial analysis of a publicly-listed company from @tbl-companies. This analysis must be based on the latest annual reports available, and complemented with additional information when needed. The final project is titled \"Financial Accounting for Managers: Final Project\" It will be published in November 2024 and will be available to all students at the end of the project.\n"
     ]
    }
   ],
   "source": [
    "#%pip install nltk --quiet\n",
    "\n",
    "# Create a summarisation pipeline\n",
    "# You can choose another summarisation model from Hugging Face if you prefer\n",
    "summariser = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "# Helper function to summarise text in chunks\n",
    "def summarise_text(text, max_chunk=1000, summary_max_length=200, summary_min_length=50):\n",
    "    \"\"\"Summarise a long piece of text by splitting it into chunks.\"\"\"\n",
    "    # Split text into sentences to avoid cutting sentences in half\n",
    "    import nltk\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    current_len = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # If adding the next sentence would exceed the max_chunk length, start a new chunk\n",
    "        if current_len + len(sentence) > max_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + ' '\n",
    "            current_len = len(sentence)\n",
    "        else:\n",
    "            current_chunk += sentence + ' '\n",
    "            current_len += len(sentence)\n",
    "\n",
    "    # Append the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summariser(chunk, max_length=summary_max_length, min_length=summary_min_length, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # If there is more than one summary, summarise the summaries\n",
    "    if len(summaries) > 1:\n",
    "        combined_text = \" \".join(summaries)\n",
    "        # Truncate to 2000 characters (adjust as needed)\n",
    "        combined_text = combined_text[:2000]\n",
    "        combined_summary = summariser(combined_text, max_length=summary_max_length, min_length=summary_min_length, do_sample=False)[0]['summary_text']\n",
    "        return combined_summary\n",
    "    else:\n",
    "        return summaries[0]\n",
    "\n",
    "# Generate summaries of the instructions and topics\n",
    "instructions_summary = summarise_text(instructions_text, max_chunk=1500, summary_max_length=250, summary_min_length=80)\n",
    "topics_summary = summarise_text(topics_text, max_chunk=1500, summary_max_length=250, summary_min_length=80)\n",
    "\n",
    "print(\"Summary of project instructions:\\n\", instructions_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06506e6",
   "metadata": {},
   "source": [
    "## 4. Generate a draft report using a text generation model\n",
    "\n",
    "With our summaries ready, we can now build a prompt that instructs the language model to generate a structured financial analysis report.\n",
    "We'll use the FLAN‑T5 model (`google/flan-t5-base`) for text generation.  You can swap in `google/flan-t5-large` or any other instruction‑tuned model depending on the resources you have available.\n",
    "\n",
    "The prompt explicitly lists the six sections required in the project: (1) company description and recent news, (2) financing situation, (3) investing situation, (4) operating performance, (5) forecast, and (6) conclusions.  The model uses the summaries we generated to inform its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6702cc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04808abfdf52461a80d0d553df3c9122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3588f84adfe4f9e92f3a4edb59b979c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e310393d3b5148ba9a214bbe8715240c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db4b7028ef4452a9da5cee16e061fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca3230bb2074ea6a90961ac7d2346cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bcdb6a420a4131af1ba7ce3c1dea0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd409e2b3af4c91b5453ed13d2f0e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated report:\n",
      " Summary of a financial analysis of a publicly listed company.\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline using FLAN‑T5\n",
    "report_generator = pipeline('text2text-generation', model='google/flan-t5-base')\n",
    "\n",
    "# Build the prompt for report generation\n",
    "prompt = f\"\"\"\n",
    "You are a financial analyst asked to write a detailed report for a publicly listed company.\n",
    "The report should follow the structure below and be based on the provided summaries.\n",
    "\n",
    "Project instructions summary:\n",
    "{instructions_summary}\n",
    "\n",
    "Lecture topics summary:\n",
    "{topics_summary}\n",
    "\n",
    "Write a report with the following sections:\n",
    "1. Company description and recent news\n",
    "2. Analysis of the financing situation\n",
    "3. Analysis of the investing situation\n",
    "4. Analysis of operating performance\n",
    "5. Forecast and assumptions\n",
    "6. Conclusions and recommendations\n",
    "\n",
    "For each section, provide clear, concise paragraphs explaining the key points, incorporating information from the summaries where relevant.\n",
    "End the report with a brief discussion of potential limitations and risks associated with the analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the report\n",
    "report_output = report_generator(prompt, max_length=1024, num_beams=4, do_sample=False)[0]['generated_text']\n",
    "\n",
    "print(\"Generated report:\\n\", report_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated report to a text file for later editing or inclusion in your project\n",
    "output_path = '/content/generated_financial_report.txt'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_output)\n",
    "\n",
    "print(f\"Report saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebce0e5",
   "metadata": {},
   "source": [
    "## 5. Next steps\n",
    "\n",
    "The generated report serves as a draft that you can refine and expand.  Consider the following when working on your project:\n",
    "\n",
    "- **Validate facts and figures**: The language model synthesises information from the summaries but does not access real‑time financial data. You should cross‑check numbers and company‑specific details using the latest annual reports and reliable news sources.\n",
    "- **Incorporate charts and tables**: Supplement the textual analysis with your own calculations in Excel and visualise key metrics (e.g., debt structure, revenue breakdown, forecast scenarios).\n",
    "- **Tailor the narrative**: Adjust the tone and focus of the report to match the intended audience (investors, creditors, etc.) and highlight insights that are most relevant.\n",
    "- **Document your assumptions**: Clearly state the assumptions underlying your forecast and discuss how sensitive your conclusions are to changes in these assumptions.\n",
    "\n",
    "By combining structured financial analysis with AI‑powered drafting, you can accelerate the reporting process while maintaining a high level of clarity and professionalism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb408bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Upload the annual report PDF file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "import os\n",
    "pdf_filename = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded file: {pdf_filename}\")\n",
    "\n",
    "# Step 2: Extract text from the PDF file\n",
    "!pip install -q pdfplumber\n",
    "import pdfplumber\n",
    "\n",
    "pdf_text = \"\"\n",
    "with pdfplumber.open(pdf_filename) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        pdf_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(\"First 1000 characters of extracted text:\")\n",
    "print(pdf_text[:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
